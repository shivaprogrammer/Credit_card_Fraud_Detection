{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1.  Project Overview\n",
        "Credit card fraud is a critical issue for banks and financial institutions, leading to **billions of dollars in losses annually**.  \n",
        "This project aims to develop a **machine learning pipeline** for fraud detection using the **Kaggle Credit Card Fraud Dataset**.\n",
        "\n",
        "The workflow includes:\n",
        "- **Data Preprocessing & Exploratory Data Analysis (EDA)**  \n",
        "- **Class Imbalance Handling** using SMOTE & SMOTEENN  \n",
        "- **Model Training & Hyperparameter Tuning** (Logistic Regression, Random Forest, XGBoost)  \n",
        "- **Model Evaluation & Comparison** using ROC-AUC, PR-AUC, Precision, Recall, and F1-score  \n",
        "- **Model Selection** for deployment"
      ],
      "metadata": {
        "id": "--iZ_x59uYBL"
      },
      "id": "--iZ_x59uYBL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.  Import Required Libraries\n",
        "\n",
        "We begin by importing all the **necessary libraries** for data manipulation, visualization, modeling, and evaluation:\n",
        "\n",
        "- **Pandas, Numpy** â†’ Data handling and numerical operations  \n",
        "- **Matplotlib, Seaborn** â†’ Visualization  \n",
        "- **Scikit-learn** â†’ Preprocessing, model training, evaluation  \n",
        "- **XGBoost** â†’ Gradient boosting model for structured data  \n",
        "- **Imbalanced-learn** â†’ Resampling techniques (SMOTE, SMOTEENN)  \n",
        "- **Joblib** â†’ Model saving and loading  \n",
        "- **SHAP** â†’ Model interpretability (optional)  "
      ],
      "metadata": {
        "id": "AjiMIKPguxlo"
      },
      "id": "AjiMIKPguxlo"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix,\n",
        "    roc_auc_score, roc_curve, precision_recall_curve,\n",
        "    average_precision_score\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.combine import SMOTEENN\n",
        "import joblib\n",
        "\n",
        "# Optional SHAP\n",
        "try:\n",
        "    import shap\n",
        "    shap_available = True\n",
        "except ImportError:\n",
        "    shap_available = False\n",
        "    print(\" SHAP not installed. Skipping explainability.\")"
      ],
      "metadata": {
        "id": "k_OTRZnuuWyR"
      },
      "id": "k_OTRZnuuWyR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.  Dataset Overview\n",
        "- **Total Transactions**: 284,807  \n",
        "- **Fraudulent Transactions**: 492 (~0.17%) â†’ **severe imbalance**  \n",
        "- **Features**:  \n",
        "  - 28 anonymized PCA-transformed features (`V1`â€“`V28`)  \n",
        "  - `Time` â†’ seconds elapsed since first transaction  \n",
        "  - `Amount` â†’ transaction value  \n",
        "- **Target Variable**: `Class`  \n",
        "  - `0` â†’ Non-Fraud  \n",
        "  - `1` â†’ Fraud"
      ],
      "metadata": {
        "id": "gDIRaKclv2vV"
      },
      "id": "gDIRaKclv2vV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.  Data Splitting\n",
        "We perform a **stratified split** to preserve fraud/non-fraud ratio:\n",
        "\n",
        "- **Train Set (80%)**: 227,845 transactions  \n",
        "- **Test Set (20%)**: 56,962 transactions  \n",
        "\n",
        "Fraud cases:  \n",
        "- Train â†’ 394 frauds  \n",
        "- Test â†’ 98 frauds  \n",
        "\n",
        "The train set is further split into:\n",
        "- **Training subset (80%)**  \n",
        "- **Validation subset (20%)**  \n",
        "\n"
      ],
      "metadata": {
        "id": "Ur_YN6q5wji-"
      },
      "id": "Ur_YN6q5wji-"
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"C:/Users/tiwar/OneDrive/Desktop/credit_card_fraud/data/creditcard.csv\")\n",
        "\n",
        "# Split into train and test\n",
        "train_df, test_df = train_test_split(\n",
        "    df, test_size=0.2, stratify=df[\"Class\"], random_state=42\n",
        ")\n",
        "\n",
        "train_df.to_csv(\"creditcard_train.csv\", index=False)\n",
        "test_df.to_csv(\"creditcard_test.csv\", index=False)\n",
        "\n",
        "print(\" Data split completed!\")\n",
        "print(\"Train shape:\", train_df.shape, \"Test shape:\", test_df.shape)\n",
        "print(\"Fraud cases in Train:\", train_df[\"Class\"].sum())\n",
        "print(\"Fraud cases in Test:\", test_df[\"Class\"].sum())\n",
        "\n",
        "df = train_df.copy()"
      ],
      "metadata": {
        "id": "ZmWhJPGQwXfH"
      },
      "id": "ZmWhJPGQwXfH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.  Exploratory Data Analysis (EDA)\n",
        "\n",
        "### 4.1 Class Imbalance\n",
        "- Fraud = **0.17%**  \n",
        "- Non-fraud = **99.83%**\n",
        "\n",
        " **Insight**: Models trained naively will predict almost everything as *non-fraud*.  \n",
        "Thus, we must use **resampling techniques** and imbalance-aware metrics."
      ],
      "metadata": {
        "id": "o2USYxWXw46I"
      },
      "id": "o2USYxWXw46I"
    },
    {
      "cell_type": "code",
      "source": [
        "# Class distribution\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x=\"Class\", data=df)\n",
        "plt.title(\"Fraud (1) vs Non-Fraud (0) Distribution (Train)\")\n",
        "plt.show()\n",
        "\n",
        "# Pie chart\n",
        "plt.figure(figsize=(6,6))\n",
        "df[\"Class\"].value_counts().plot.pie(\n",
        "    autopct='%1.2f%%',\n",
        "    labels=[\"Non-Fraud\",\"Fraud\"],\n",
        "    colors=[\"skyblue\",\"red\"]\n",
        ")\n",
        "plt.title(\"Fraud vs Non-Fraud (Pie Chart) (Train)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W67GQB8Nw4Ky"
      },
      "id": "W67GQB8Nw4Ky",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Transaction Amount Distribution\n",
        "- **Linear Scale**:  \n",
        "  - Highly right-skewed.  \n",
        "  - Most transactions are **small (< $100)** ..\n",
        "  - Some extreme values (~$25,000).  \n",
        "\n",
        "- **Log Scale**:  \n",
        "  - Distribution reveals multiple peaks.  \n",
        "  - Indicates popular transaction ranges (e.g., small daily purchases, medium recurring payments).\n",
        "\n",
        " **Insight**: Fraudulent transactions often cluster at **small values**, possibly to avoid detection.\n"
      ],
      "metadata": {
        "id": "TtclY0mxxTWr"
      },
      "id": "TtclY0mxxTWr"
    },
    {
      "cell_type": "code",
      "source": [
        "# Amount distribution\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df[\"Amount\"], bins=100, kde=True)\n",
        "plt.title(\"Transaction Amount Distribution (Train)\")\n",
        "plt.show()\n",
        "\n",
        "# Amount distribution log scale\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df[\"Amount\"], bins=100, log_scale=True, kde=True)\n",
        "plt.title(\"Transaction Amount Distribution (Log Scale) (Train)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8nD0C3FRxaLz"
      },
      "id": "8nD0C3FRxaLz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Amount Distribution by Class\n",
        "- Non-fraud transactions span entire value range.  \n",
        "- Fraud transactions concentrated below **$500**.  \n",
        "\n",
        " **Insight**: Fraudsters seem to prefer smaller charges, likely to bypass suspicion."
      ],
      "metadata": {
        "id": "CdAY2Kchx1k6"
      },
      "id": "CdAY2Kchx1k6"
    },
    {
      "cell_type": "code",
      "source": [
        "# Amount by class\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.histplot(df[df[\"Class\"]==0][\"Amount\"], bins=50, color=\"blue\", label=\"Non-Fraud\", alpha=0.6)\n",
        "sns.histplot(df[df[\"Class\"]==1][\"Amount\"], bins=50, color=\"red\", label=\"Fraud\", alpha=0.6)\n",
        "plt.legend()\n",
        "plt.title(\"Amount Distribution by Class (Train)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UTkyLIcPx4XU"
      },
      "id": "UTkyLIcPx4XU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Transaction Time Analysis\n",
        "- Non-fraud transactions:  \n",
        "  - Follow a clear **daily cycle**.  \n",
        "  - Peaks during **business hours**.  \n",
        "\n",
        "- Fraud transactions:  \n",
        "  - More **randomly distributed** across time.  \n",
        "  - Appear during unusual hours.  \n",
        "\n",
        " **Insight**: Fraud may be linked to **off-peak times** where monitoring is weaker."
      ],
      "metadata": {
        "id": "Zs53yM4Lx8e0"
      },
      "id": "Zs53yM4Lx8e0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Transaction time distribution\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.histplot(df[df[\"Class\"]==0][\"Time\"], bins=50, color=\"blue\", label=\"Non-Fraud\", alpha=0.6)\n",
        "sns.histplot(df[df[\"Class\"]==1][\"Time\"], bins=50, color=\"red\", label=\"Fraud\", alpha=0.6)\n",
        "plt.legend()\n",
        "plt.title(\"Transaction Time Distribution by Class (Train)\")\n",
        "plt.xlabel(\"Time (seconds since first transaction)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5qmc3z-2x707"
      },
      "id": "5qmc3z-2x707",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5 Correlation Heatmap\n",
        "- Features (`V1â€“V28`) are **decorrelated** due to PCA transformation.  \n",
        "- No strong correlation with `Class`.  \n",
        "\n",
        " **Insight**: Confirms dataset is anonymized safely (no data leakage)."
      ],
      "metadata": {
        "id": "By1GG1ItyGj4"
      },
      "id": "By1GG1ItyGj4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation heatmap\n",
        "plt.figure(figsize=(12,8))\n",
        "corr = df.corr()\n",
        "sns.heatmap(corr, cmap=\"coolwarm\", center=0)\n",
        "plt.title(\"Correlation Heatmap (Train)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mDBzs3OMyIwj"
      },
      "id": "mDBzs3OMyIwj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.6 PCA Feature Distributions\n",
        "- Example: **V1â€“V3 distributions** differ slightly for fraud vs non-fraud.  \n",
        "- Fraudulent cases tend to deviate more strongly in **V2 & V3**.  \n",
        "\n",
        "**Insight**: Even anonymized PCA features capture **fraud-specific anomalies**."
      ],
      "metadata": {
        "id": "6FIQ_9t_yNOK"
      },
      "id": "6FIQ_9t_yNOK"
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA feature distributions\n",
        "fraud = df[df[\"Class\"]==1]\n",
        "nonfraud = df[df[\"Class\"]==0]\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "for col in [\"V1\",\"V2\",\"V3\"]:\n",
        "    sns.kdeplot(nonfraud[col], label=f\"{col} Non-Fraud\", fill=True, alpha=0.3)\n",
        "    sns.kdeplot(fraud[col], label=f\"{col} Fraud\", fill=True, alpha=0.3)\n",
        "plt.title(\"Distribution of PCA Features (V1â€“V3) by Class (Train)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "As8F2CkiyMqe"
      },
      "id": "As8F2CkiyMqe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  5: Feature Engineering\n",
        "\n",
        "- Split features (`X`) and target (`y` = Class).  \n",
        "- Standardized **Time** and **Amount** using `StandardScaler` to match PCA feature scales.  \n",
        "- Applied scaling to both training and test sets.  \n",
        "- Further split training data into **train (64%)** and **validation (16%)** for model tuning.  \n",
        "- Final test set (20%) is saved as `creditcard_test.csv` for unbiased evaluation.\n"
      ],
      "metadata": {
        "id": "iKT0HlAIzEt1"
      },
      "id": "iKT0HlAIzEt1"
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(\"Class\", axis=1)\n",
        "y = df[\"Class\"]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X[[\"Time\", \"Amount\"]] = scaler.fit_transform(X[[\"Time\", \"Amount\"]])\n",
        "\n",
        "X_test_full = test_df.drop(\"Class\", axis=1).copy()\n",
        "y_test_full = test_df[\"Class\"]\n",
        "X_test_full[[\"Time\", \"Amount\"]] = scaler.transform(X_test_full[[\"Time\", \"Amount\"]])\n",
        "\n",
        "# Train-test split for model training\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Train shape:\", X_train.shape, \"Validation shape:\", X_val.shape)"
      ],
      "metadata": {
        "id": "uUhDE5nezSPk"
      },
      "id": "uUhDE5nezSPk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.  Handling Class Imbalance\n",
        "We test two approaches:  \n",
        "\n",
        "1. **SMOTE (Synthetic Minority Oversampling Technique)**  \n",
        "   - Creates synthetic fraud samples.  \n",
        "   - Ensures **balanced 50:50 dataset**.  \n",
        "\n",
        "2. **SMOTEENN (SMOTE + Edited Nearest Neighbors)**  \n",
        "   - Adds oversampling + cleaning of noisy samples.  \n",
        "   - Results in slightly fewer samples but reduces overlap.  \n",
        "\n",
        " **Insight**:  \n",
        "- **SMOTE** â†’ preserves more samples.  \n",
        "- **SMOTEENN** â†’ creates cleaner training sets."
      ],
      "metadata": {
        "id": "eJ6wR0iazspg"
      },
      "id": "eJ6wR0iazspg"
    },
    {
      "cell_type": "code",
      "source": [
        "strategies = {\n",
        "    \"SMOTE\": SMOTE(random_state=42),\n",
        "    \"SMOTEENN\": SMOTEENN(random_state=42)\n",
        "}"
      ],
      "metadata": {
        "id": "DZHYXl6EzygY"
      },
      "id": "DZHYXl6EzygY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.  Models & Hyperparameter Tuning\n",
        "We test 3 classifiers:  \n",
        "\n",
        "1. **Logistic Regression**  \n",
        "   - Linear, interpretable.  \n",
        "   - Class-weight balancing applied.  \n",
        "\n",
        "2. **Random Forest**  \n",
        "   - Bagging ensemble, robust to imbalance.  \n",
        "   - Tuned: `n_estimators`, `max_depth`, `max_features`.  \n",
        "\n",
        "3. **XGBoost**  \n",
        "   - Gradient boosting, handles complex patterns.  \n",
        "   - Tuned: `n_estimators`, `max_depth`, `learning_rate`.  \n",
        "\n",
        " **Search Method**: RandomizedSearchCV with **ROC-AUC scoring**."
      ],
      "metadata": {
        "id": "iGb-pSbK0AqX"
      },
      "id": "iGb-pSbK0AqX"
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000, class_weight=\"balanced\"),\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
        "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n",
        "}\n",
        "\n",
        "param_grids = {\n",
        "    \"Random Forest\": {\n",
        "        \"n_estimators\": [100, 200],\n",
        "        \"max_depth\": [10, None],\n",
        "        \"max_features\": [\"sqrt\", \"log2\"]\n",
        "    },\n",
        "    \"XGBoost\": {\n",
        "        \"n_estimators\": [100, 200],\n",
        "        \"max_depth\": [3, 6],\n",
        "        \"learning_rate\": [0.05, 0.1]\n",
        "    }\n",
        "}\n",
        "\n",
        "results = {}\n",
        "all_fpr, all_tpr, all_auc, all_prec_rec = {}, {}, {}, {}"
      ],
      "metadata": {
        "id": "cUPQqjqRz3ck"
      },
      "id": "cUPQqjqRz3ck",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Evaluation Metrics\n",
        "Why standard accuracy is misleading:  \n",
        "- Always predicting *non-fraud* yields ~99.8% accuracy but **zero fraud detection**.  \n",
        "\n",
        "Thus, we focus on:  \n",
        "- **ROC-AUC** â†’ overall separability.  \n",
        "- **PR-AUC** â†’ better for imbalanced datasets.  \n",
        "- **Precision** â†’ of predicted frauds, how many are correct.  \n",
        "- **Recall** â†’ of true frauds, how many are detected.  \n",
        "- **F1-score** â†’ harmonic mean of Precision & Recall."
      ],
      "metadata": {
        "id": "udJPPg-Y0Pk_"
      },
      "id": "udJPPg-Y0Pk_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.  Results & Insights\n",
        "\n",
        "### 9.1 Logistic Regression\n",
        "- **Recall ~0.87** â†’ detects most frauds.  \n",
        "- **Precision ~0.05** â†’ many false positives.  \n",
        "- F1 ~0.10.  \n",
        "\n",
        " **Insight**: Good at catching fraud, but too many false alarms â†’ not practical."
      ],
      "metadata": {
        "id": "4CkKYNvS0iHy"
      },
      "id": "4CkKYNvS0iHy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.2 Random Forest\n",
        "- With **SMOTE**:  \n",
        "  - Precision = **0.64**  \n",
        "  - Recall = 0.88  \n",
        "  - F1 = **0.74**  \n",
        "  - ROC-AUC = **0.977**  \n",
        "  - PR-AUC = 0.786  \n",
        "\n",
        "- With **SMOTEENN**:  \n",
        "  - Precision = 0.59  \n",
        "  - Recall = 0.88  \n",
        "  - F1 = 0.70  \n",
        "  - ROC-AUC = **0.984 (best overall)**  \n",
        "\n",
        " **Insight**: Random Forest achieves the **best trade-off between recall and precision**."
      ],
      "metadata": {
        "id": "h0DDibHH0r9q"
      },
      "id": "h0DDibHH0r9q"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.3 XGBoost\n",
        "- Recall ~0.89 (very high).  \n",
        "- Precision ~0.19â€“0.23 (moderate).  \n",
        "- F1 ~0.31.  \n",
        "- ROC-AUC ~0.969â€“0.970.  \n",
        "\n",
        " **Insight**: Very strong recall but tends to **over-flag frauds**."
      ],
      "metadata": {
        "id": "3jPKhbCZ08CC"
      },
      "id": "3jPKhbCZ08CC"
    },
    {
      "cell_type": "code",
      "source": [
        "for strat_name, sampler in strategies.items():\n",
        "    print(f\"\\n========== Using {strat_name} ==========\")\n",
        "    X_train_res, y_train_res = sampler.fit_resample(X_train, y_train)\n",
        "    print(\"Resampled shape:\", X_train_res.shape, y_train_res.value_counts().to_dict())\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\nðŸ”¹ Training {name} with {strat_name}...\")\n",
        "\n",
        "        if name in param_grids:\n",
        "            sample_size = min(50000, len(X_train_res))\n",
        "            X_sample = X_train_res.sample(sample_size, random_state=42)\n",
        "            y_sample = y_train_res.loc[X_sample.index]\n",
        "\n",
        "            search = RandomizedSearchCV(\n",
        "                model,\n",
        "                param_distributions=param_grids[name],\n",
        "                n_iter=3,\n",
        "                scoring=\"roc_auc\",\n",
        "                cv=3,\n",
        "                n_jobs=1,\n",
        "                random_state=42\n",
        "            )\n",
        "            search.fit(X_sample, y_sample)\n",
        "            model = search.best_estimator_\n",
        "            print(\"Best Params:\", search.best_params_)\n",
        "        else:\n",
        "            model.fit(X_train_res, y_train_res)\n",
        "\n",
        "        # Predictions\n",
        "        y_pred = model.predict(X_val)\n",
        "        y_prob = model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "        # Metrics\n",
        "        auc = roc_auc_score(y_val, y_prob)\n",
        "        pr_auc = average_precision_score(y_val, y_prob)\n",
        "        print(classification_report(y_val, y_pred))\n",
        "        print(\"ROC-AUC:\", auc)\n",
        "        print(\"PR-AUC:\", pr_auc)\n",
        "\n",
        "        # Save model\n",
        "        filename = f\"{name.replace(' ', '_').lower()}_{strat_name.lower()}_fraud_model.pkl\"\n",
        "        joblib.dump(model, filename)\n",
        "\n",
        "        # Store results\n",
        "        results[(name, strat_name)] = {\"auc\": auc, \"pr_auc\": pr_auc}\n",
        "\n",
        "        # ROC + PR curves\n",
        "        fpr, tpr, _ = roc_curve(y_val, y_prob)\n",
        "        prec, rec, _ = precision_recall_curve(y_val, y_prob)\n",
        "        all_fpr[(name, strat_name)] = fpr\n",
        "        all_tpr[(name, strat_name)] = tpr\n",
        "        all_auc[(name, strat_name)] = auc\n",
        "        all_prec_rec[(name, strat_name)] = (prec, rec)"
      ],
      "metadata": {
        "id": "haPXVO7r0OUT"
      },
      "id": "haPXVO7r0OUT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10.  ROC & PR Curves\n",
        "\n",
        "- **ROC Curve**: All models >0.96 AUC.  \n",
        "  - Random Forest (SMOTE) shows best curve.  \n",
        "\n",
        "- **PR Curve**: More realistic under imbalance.  \n",
        "  - Random Forest maintains **highest precision at high recall**.  \n",
        "  - Logistic Regression collapses quickly in precision.  \n",
        "\n",
        " **Insight**: PR-AUC is a better reflection of fraud detection performance."
      ],
      "metadata": {
        "id": "5lAI3p9-1SAQ"
      },
      "id": "5lAI3p9-1SAQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# ROC curves\n",
        "plt.figure(figsize=(8,6))\n",
        "for (name, strat), fpr in all_fpr.items():\n",
        "    plt.plot(fpr, all_tpr[(name, strat)], label=f\"{name}-{strat} (AUC={all_auc[(name, strat)]:.3f})\")\n",
        "plt.plot([0,1],[0,1],'k--')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curves (All Models)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Precisionâ€“Recall curves\n",
        "plt.figure(figsize=(8,6))\n",
        "for (name, strat), (prec, rec) in all_prec_rec.items():\n",
        "    plt.plot(rec, prec, label=f\"{name}-{strat}\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precisionâ€“Recall Curves (All Models)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vjkyum8q1WrM"
      },
      "id": "vjkyum8q1WrM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Feature Importance and Explainability  \n",
        "\n",
        "To better understand our fraud detection models, we analyze **which features drive predictions** using two complementary methods:  \n",
        "\n",
        "1. **Feature Importance** (from Random Forest & XGBoost)  \n",
        "2. **SHAP (SHapley Additive exPlanations)**  \n",
        "\n",
        "---\n",
        "\n",
        "###  Feature Importance (Random Forest / XGBoost)  \n",
        "\n",
        "Feature importance measures the contribution of each feature to the modelâ€™s predictions.  \n",
        "\n",
        "- **XGBoost (SMOTE & SMOTEENN)** shows that **V14** is by far the most influential feature.  \n",
        "- Other important features include **V10, V12, V4, V8, and V13**.  \n",
        "- The importance decreases quickly after the top 3â€“4 features, meaning a few features carry most of the predictive power.  \n",
        "\n",
        "**Insights:**  \n",
        "- **V14** consistently stands out as the strongest fraud indicator.  \n",
        "- Features like **V10, V12, and V4** also play important roles.  \n",
        "- Both Random Forest and XGBoost agree on the dominance of **V14**, making it critical for fraud detection.  \n"
      ],
      "metadata": {
        "id": "KaqkzLnR2b_9"
      },
      "id": "KaqkzLnR2b_9"
    },
    {
      "cell_type": "code",
      "source": [
        "for (name, strat), _ in results.items():\n",
        "    if \"random_forest\" in name.lower() or \"xgboost\" in name.lower():\n",
        "        model = joblib.load(f\"{name.replace(' ', '_').lower()}_{strat.lower()}_fraud_model.pkl\")\n",
        "        if hasattr(model, \"feature_importances_\"):\n",
        "            feat_imp = pd.DataFrame({\n",
        "                \"Feature\": X.columns,\n",
        "                \"Importance\": model.feature_importances_\n",
        "            }).sort_values(by=\"Importance\", ascending=False).head(10)\n",
        "\n",
        "            plt.figure(figsize=(8,6))\n",
        "            sns.barplot(x=\"Importance\", y=\"Feature\", data=feat_imp)\n",
        "            plt.title(f\"Top 10 Feature Importances - {name} ({strat})\")\n",
        "            plt.show()"
      ],
      "metadata": {
        "id": "ZZC2Edxz2g3d"
      },
      "id": "ZZC2Edxz2g3d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  SHAP Explainability  \n",
        "\n",
        "While feature importance gives a global view, **SHAP** explains predictions at both the global and local level.  \n",
        "\n",
        "- The **beeswarm plot** shows how each feature impacts fraud probability.  \n",
        "- **V4, V14, V8, and V13** have the largest SHAP contributions.  \n",
        "- The plot also shows **directionality**:  \n",
        "  - Positive SHAP values â†’ push prediction toward *fraud*.  \n",
        "  - Negative SHAP values â†’ push prediction toward *non-fraud*.  \n",
        "\n",
        "**Insights:**  \n",
        "- SHAP confirms **V14 and V4** as critical fraud predictors.  \n",
        "- High values of **V14** strongly increase fraud likelihood.  \n",
        "- SHAP helps ensure **transparency and trust**, which is essential in financial fraud detection.  "
      ],
      "metadata": {
        "id": "Vc8PHYRm2ivY"
      },
      "id": "Vc8PHYRm2ivY"
    },
    {
      "cell_type": "code",
      "source": [
        "if shap_available:\n",
        "    best_model_name, best_resample = max(results, key=lambda x: results[x][\"auc\"])\n",
        "    print(f\"\\n Best Model: {best_model_name} ({best_resample}) with AUC={results[(best_model_name, best_resample)]['auc']:.4f}\")\n",
        "\n",
        "    best_model = joblib.load(f\"{best_model_name.replace(' ', '_').lower()}_{best_resample.lower()}_fraud_model.pkl\")\n",
        "    explainer = shap.Explainer(best_model, X_val)\n",
        "    shap_values = explainer(X_val[:200])\n",
        "\n",
        "    # Fix for multi-output explanation shape\n",
        "    if len(shap_values.values.shape) == 3:\n",
        "        shap_values = shap_values[:, :, 1]\n",
        "\n",
        "    shap.plots.beeswarm(shap_values)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "OuLH1nyk2oKO"
      },
      "id": "OuLH1nyk2oKO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Save Scaler + Metadata\n",
        "joblib.dump(scaler, \"scaler.pkl\")\n",
        "joblib.dump(list(X.columns), \"feature_names.pkl\")\n",
        "print(\"\\n Models, scaler, metadata saved successfully!\")\n",
        "print(\"Test set saved as 'creditcard_test.csv' for Streamlit Compare Models.\")"
      ],
      "metadata": {
        "id": "XF9MQZIh2wrn"
      },
      "id": "XF9MQZIh2wrn",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}